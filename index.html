<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- =========================
       Primary Meta Tags (SEO)
  ========================== -->
  <meta name="title" content="LLM Reviewers: When They Can Surpass Humans, and What Still Fails (UROP 2025)">
  <meta name="description" content="A benchmark and pipeline to compare LLM-generated peer reviews against human reviews across alignment, groundedness, rigor, scholarly context, actionability, prioritization, and conduct.">
  <meta name="keywords" content="peer review, LLM evaluation, LLM reviewers, benchmarking, rubric, groundedness, specificity, scholarly context, LLM-as-a-judge, review quality, NeurIPS, ICLR">
  <meta name="author" content="UROP 2025 LLM Reviewers Team (UIUC)">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- =========================
       Open Graph / Social
  ========================== -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="UROP 2025 (UIUC) — LLMs as Reviewers">
  <meta property="og:title" content="LLM Reviewers: When They Can Surpass Humans, and What Still Fails">
  <meta property="og:description" content="A benchmark and pipeline to compare LLM-generated peer reviews against human reviews across alignment, groundedness, rigor, scholarly context, actionability, prioritization, and conduct.">
  <!-- TODO: Update to your GitHub Pages URL -->
  <meta property="og:url" content="https://YOUR_GITHUB_USERNAME.github.io/YOUR_REPO/">
  <!-- TODO: Provide 1200x630 image at this path -->
  <meta property="og:image" content="https://YOUR_GITHUB_USERNAME.github.io/YOUR_REPO/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="LLMs as Reviewers Benchmark — Preview">
  <meta property="article:published_time" content="2025-12-30T00:00:00.000Z">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="peer review">
  <meta property="article:tag" content="LLM evaluation">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: add handles or remove -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <meta name="twitter:title" content="LLM Reviewers: When They Can Surpass Humans, and What Still Fails">
  <meta name="twitter:description" content="Benchmarking LLM reviewers vs humans with metrics for relevance, specificity, synthesize-ability, judge-based scoring, and LLM-detection.">
  <meta name="twitter:image" content="https://YOUR_GITHUB_USERNAME.github.io/YOUR_REPO/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="LLMs as Reviewers Benchmark — Preview">

  <!-- Academic meta (fill when ready) -->
  <meta name="citation_title" content="LLM Reviewers: When They Can Surpass Humans, and What Still Fails">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="Preprint / In Progress">
  <meta name="citation_pdf_url" content="https://YOUR_GITHUB_USERNAME.github.io/YOUR_REPO/static/pdfs/paper.pdf">

  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>LLM Reviewers: When They Can Surpass Humans, and What Still Fails</title>

  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- CSS -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- JS -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "LLM Reviewers: When They Can Surpass Humans, and What Still Fails",
    "description": "A benchmark and pipeline to compare LLM-generated peer reviews against human reviews across alignment, groundedness, rigor, scholarly context, actionability, prioritization, and conduct.",
    "author": [
      { "@type": "Person", "name": "UROP 2025 LLM Reviewers Team" }
    ],
    "datePublished": "2025-12-30",
    "publisher": { "@type": "Organization", "name": "Preprint / In Progress" },
    "url": "https://YOUR_GITHUB_USERNAME.github.io/YOUR_REPO/",
    "image": "https://YOUR_GITHUB_USERNAME.github.io/YOUR_REPO/static/images/social_preview.png",
    "keywords": ["peer review", "LLM evaluation", "LLM reviewers", "benchmarking", "rubric", "groundedness", "specificity", "LLM-as-a-judge"],
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/"
  }
  </script>

  <style>
    /* Small additions for readability (keeps template CSS intact) */
    .section-title { margin-bottom: 0.5rem; }
    .pill {
      display: inline-block;
      padding: 0.25rem 0.6rem;
      border-radius: 999px;
      border: 1px solid rgba(0,0,0,0.12);
      margin: 0.2rem 0.25rem 0 0;
      font-size: 0.85rem;
      background: rgba(37,99,235,0.06);
    }
    .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; }
    .callout {
      border-left: 4px solid #2563eb;
      padding: 0.9rem 1rem;
      background: rgba(37,99,235,0.06);
      border-radius: 10px;
    }
    .tiny { font-size: 0.92rem; opacity: 0.85; }
    .table-wrap { overflow-x:auto; }
    .nav-links a { margin: 0 0.5rem; }
  </style>
</head>

<body>

  <!-- Scroll to Top Button (index.js should provide scrollToTop()) -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- Top mini-nav -->
  <section class="section" style="padding-bottom:0;">
    <div class="container is-max-desktop has-text-centered nav-links">
      <a href="#overview">Overview</a> ·
      <a href="#pipeline">Pipeline</a> ·
      <a href="#datasets">Datasets</a> ·
      <a href="#methods">Methods</a> ·
      <a href="#metrics">Metrics</a> ·
      <a href="#results">Preliminary Results</a> ·
      <a href="#roadmap">Roadmap</a> ·
      <a href="#resources">Resources</a> ·
      <a href="#references">References</a> ·
      <a href="#BibTeX">BibTeX</a>
    </div>
  </section>

  <!-- HERO -->
  <section class="hero" id="overview">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              LLM Reviewers: When They Can Surpass Humans, and What Still Fails
            </h1>

            <div class="is-size-5 publication-authors">
              <!-- <span class="author-block"><a href="#" target="_blank">Duy A. Nguyen</a>,</span>
              <span class="author-block"><a href="#" target="_blank">Thanh Nho</a>,</span>
              <span class="author-block"><a href="#" target="_blank">Thanh Nguyen</a>,</span>
              <span class="author-block"><a href="#" target="_blank">Thành Trần</a>,</span>
              <span class="author-block"><a href="#" target="_blank">Tuấn Anh</a></span> -->
              <span class="author-block"><a href="#" target="_blank">UROP Students</a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Vin University 2025</span>
            </div>

            <div class="content has-text-centered" style="margin-top: 1rem;">
              <span class="pill">Relevance</span>
              <span class="pill">Specificity</span>
              <span class="pill">Synthesizing Ability</span>
              <span class="pill">LLM-as-a-Judge</span>
              <span class="pill">LLM Detector</span>
              <span class="pill">Standardization (SEA)</span>
            </div>

            <div class="column has-text-centered" style="margin-top: 0.8rem;">
              <div class="publication-links">

                <!-- TODO: Update links when ready -->
                <span class="link-block">
                  <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Technical Report</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/YOUR_GITHUB_USERNAME/YOUR_REPO" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="#results" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-chart-line"></i></span>
                    <span>Results</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="#datasets" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-database"></i></span>
                    <span>Datasets</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="#references" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-book"></i></span>
                    <span>References</span>
                  </a>
                </span>

              </div>
            </div>

            <div class="callout has-text-left" style="margin-top: 1.2rem;">
              <strong>Goal.</strong>
              Human reviews are used as a reference point—not the finish line. We use controlled comparisons to answer two questions:
              <ol style="margin-top:0.5rem;padding-left:0.75rem">
                <li><strong>Where can LLM reviewers be strong enough to match or even surpass humans?</strong>
                  (e.g., broader coverage, sharper writing, better recall of standard checks, stronger cross-paper contextualization).</li>
                <li><strong>Where do LLM reviewers still fail, and what interventions improve them?</strong>
                  (e.g., grounding/hallucinations, prioritization of major issues, actionability, and guideline compliance).</li>
              </ol>
              We operationalize these questions with a standardized generation pipeline (SEA) and multiple evaluation views:
              reference-based metrics, rubric-based judging, external grounding checks, and human-likeness signals.
            </div>

            <p class="tiny has-text-centered" style="margin-top: 0.8rem;">
              This page summarizes our ongoing work; numbers are preliminary and will be updated as we scale to more metrics, papers and models.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- TEASER: pipeline figure (preferred over video for this project) -->
  <section class="hero teaser" id="pipeline">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- TODO: Add an overview figure -->
        <!-- <iframe src="static/images/complete_pipeline.pdf" alt="Pipeline overview" style="width:100%; height:600px; border: none; border-radius:12px;" type="application/pdf"></iframe> -->
        <img src="static/images/complete_pipeline.png" alt="Pipeline overview" loading="lazy" style="width:100%; border-radius:12px;">
        <h2 class="subtitle has-text-centered" style="margin-top: 0.75rem;">
          End-to-end workflow: <span class="mono">papers → prompting/model strategy → review generation → evaluation</span>.
          We compare multiple generators (Reviewer2-style, SEA-based, SWIF2T) under shared datasets and metrics.
        </h2>
      </div>
    </div>
  </section>

  <!-- ABSTRACT -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 section-title">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              <p>
                LLMs are increasingly used to draft and critique research papers, but the real question is not whether they can
                imitate human reviewers—it is <em style="font-weight: bold;">when they can do better</em>, and <em style="font-weight: bold;">what must change when they fail</em>.
                We build a benchmark and evaluation pipeline that uses human reviews as a reference point to map the capability
                frontier of LLM reviewers across multiple dimensions: topical alignment, paper-groundedness (faithfulness),
                technical rigor, scholarly contextualization, coverage and prioritization of major issues, actionability of feedback,
                and professional conduct under top-venue norms.
                Beyond reference-based text similarity, we evaluate external grounding (whether cited concepts and methods correspond to real scholarly entities),
                rubric-based scoring via LLM-as-a-judge, and human-likeness via detector signals.
                Using multiple review-generation strategies (single-pass, standardized-output, and multi-agent variants), we identify
                where LLM reviewers can be strong and scalable, and we diagnose failure modes that require targeted interventions.
                Our results aim to provide concrete guidance on how to deploy LLM reviewers responsibly and how to improve them in
                the aspects that matter for scientific decision-making.
              </p>              
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- DATASETS -->
  <section class="section" id="datasets">
    <div class="container is-max-desktop">
      <h2 class="title is-3 section-title has-text-centered">Datasets</h2>

      <div class="columns">
        <div class="column">
          <div class="box">
            <h3 class="title is-5">NeurIPS 2023 (processed)</h3>
            <div class="content">
              <ul>
                <li><strong>Scale:</strong> 3,395 papers.</li>
                <li><strong>Artifacts:</strong> raw PDFs; full paper content in structured text (e.g., <span class="mono">.mmd</span>); reviews & comments in JSON; raw reviews in text.</li>
                <!-- <li><strong>Ops note:</strong> prototype SEA test ~ <strong>10s/paper</strong> (initial indicator).</li> -->
              </ul>
              <p class="tiny">Status: processed + standardized + evaluation scripts are being consolidated for reproducible runs.</p>
            </div>
          </div>
        </div>

        <div class="column">
          <div class="box">
            <h3 class="title is-5">ICLR 2024 (processed)</h3>
            <div class="content">
              <ul>
                <!-- <li><strong>Scope:</strong> ICLR papers spanning 2020-2024.</li>
                <li><strong>Use:</strong> larger-scale benchmarking and ablations across generation strategies.</li>
                <li><strong>Hosting:</strong> uploaded to a shared dataset repo (HuggingFace / internal mirror).</li> -->
                <li><strong>Scale:</strong> ~5,000 papers.</li>
                <li><strong>Artifacts:</strong> raw PDFs; full paper content in structured text (e.g., <span class="mono">.mmd</span>); reviews & comments in JSON; raw reviews in text.</li>
              </ul>
              <p class="tiny">
                TODO: double-check this information and update if needed.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="callout">
        <strong>Dataset format (Prompt-Review record).</strong>
        <!-- For each paper we store identifiers + metadata, an aspect prompt (6-10 numbered questions), and the generated review
        following a fixed template (Summary / Strengths / Weaknesses / Questions for Authors). This aligns with the “Reviewer2-style” evaluation setup. -->
        For each paper, besides identifiers and metadata, we store a list of actual reviews and available discussions gathered during the review process. These reviews follow a conference-specific fixed template (e.g., Summary / Strengths / Weaknesses / Questions for Authors). For current scope of the project, we use these reviews to represent the human standard during comparison.
      </div>
    </div>
  </section>

  <!-- METHODS -->
  <section class="section hero is-light" id="methods">
    <div class="container is-max-desktop">
      <h2 class="title is-3 section-title has-text-centered">Systems / Methods</h2>

      <div class="columns">
        <div class="column">
          <div class="box">
            <h3 class="title is-5">Reviewer2 (Prompt→Review)</h3>
            <div class="content">
              <ul>
                <li><strong>Parser:</strong> converts PDF into structured text (title, abstract, sections, references).</li>
                <li><strong>Prompt model (M<sub>p</sub>):</strong> generates aspect prompts (novelty, significance, clarity, experiments, etc.).</li>
                <li><strong>Review model (M<sub>r</sub>):</strong> answers prompts and composes an ICLR-style review with consistent headings.</li>
              </ul>
              <!-- <p class="tiny">
                We also benchmark non-finetuned modern models (e.g., IBM Granite 4.0 3B) against finetuned SEA-family models
                for cost/stability trade-offs.
              </p> -->
            </div>
          </div>
        </div>

        <div class="column">
          <div class="box">
            <h3 class="title is-5">SEA family (Standardization + Evaluation + Analysis)</h3>
            <div class="content">
              <ul>
                <li><strong>SEA-S:</strong> normalizes structure and formatting (enables fairer metric computation).</li>
                <li><strong>SEA-E:</strong> deeply understand papers and generate constructive reviews.</li>
                <li><strong>SEA-A:</strong> analyze the reviews and provide feedback on the reviews.</li>
              </ul>
              <!-- <p class="tiny">Known issue: throughput constraints for 7B models at scale; evaluating smaller/efficient baselines.</p> -->
            </div>
          </div>
        </div>
      </div>

      <div class="columns">
        <div class="column">
          <div class="box">
            <h3 class="title is-5">SWIF2T (multi-agent review)</h3>
            <div class="content">
              <ul>
                <li><strong>Controller:</strong> orchestrates the pipeline.</li>
                <li><strong>Investigator:</strong> researches technical concepts referenced by the paper/review target text.</li>
                <li><strong>Planner:</strong> plans steps and coverage to reduce omissions.</li>
                <li><strong>Reviewer:</strong> writes the review (label + review + reasoning in early prototype; extended to full-paper reviewing).</li>
              </ul>
            </div>
          </div>
        </div>
        
    </div>
    <div class="callout">
        <strong>Benchmarking Setup.</strong>
        For pipelines proposing trained models (SEA - E, SEA - A), we use authors' pretrains to best recreate the pipeline.
        For pipelines require no finetuned models, we use a shared LLM backbone of the same scale (e.g., IBM Granite 4.0 3B) to minimize model-different bias.
      </div>
  </div>
  </section>

  <!-- METRICS -->
  <section class="section" id="metrics">
    <div class="container is-max-desktop">
      <h2 class="title is-3 section-title has-text-centered">Current Metrics (v0) and Roadmap</h2>
      <div class="callout" style="margin-top: 1rem;margin-bottom: 1rem;">
        <strong>Important:</strong> Our current metrics are <em>proxies</em>. BLEU/ROUGE/BERTScore mostly measure similarity to
        reference human reviews and do not reliably capture correctness, groundedness, or whether a review prioritizes the most
        important issues. LLM-as-a-judge and LLM detectors are used as qualitative signals and require careful calibration.
      </div>
      <div class="columns">
        <div class="column">
          <div class="box">
            <h3 class="title is-5">1) Similarity</h3>
            <div class="content">
              <p>
                Compute text similarity between generated reviews and the corresponding human review of the same paper investigated using
                <strong>BLEU</strong>, <strong>ROUGE-1/2/L</strong> and <strong>BERTScore</strong> (max-over-references when multiple references exist).
              </p>
              <p class="tiny">
                Interpretation: ROUGE reflects content overlap; BLEU is more sensitive to wording stability (lower BLEU can still mean similar content); BERTScore is a more comprehensive semantic similarity metric.
              </p>
            </div>
          </div>
        </div>
        <div class="column">
          <div class="box">
            <h3 class="title is-5">2) Synthesizing Ability (external grounding)</h3>
            <div class="content">
              <p>
                We score <em>how well a review references real and relevant technical entities beyond the paper's text</em>.
              </p>
              <ol>
                <li>Extract proper nouns / method names / technical terms from the review.</li>
                <li>Query scholarly search (e.g., Semantic Scholar) to verify existence (binary hit).</li>
                <li><strong>Synth score</strong> = (# verified entities) / (total extracted entities).</li>
              </ol>
              <p class="tiny">
                Known challenge: disambiguation + false positives (common words, acronyms). We are iterating on entity extraction and filtering.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="columns">
        <div class="column">
          <div class="box">
            <h3 class="title is-5">3) LLM-as-a-Judge (rubric-based)</h3>
            <div class="content">
              <p>
                A meta-reviewer LLM evaluates each review under reviewer-guideline dimensions with strict JSON output.
              </p>
              <ul>
                <li>Scores (1-5): summary, claims&vidence, relation to prior work, other aspects, questions for authors, ethics.</li>
                <li>Recommendation: presence + value (1-5) when available.</li>
                <li>Diagnostics: missing items + key strengths/weaknesses + self-rated confidence.</li>
              </ul>
              <p class="tiny">
                Security: the judge treats paper/review text as untrusted input and ignores any instructions inside them.
              </p>
            </div>
          </div>
        </div>
        <div class="column">
          <div class="box">
            <h3 class="title is-5">4) LLM Detector (human-likeness proxy)</h3>
            <div class="content">
              <p>
                Mix model reviews with corresponding human reviews and run LLM detectors (e.g., GPTZero / DetectGPT / other detectors).
                Report relative detectability (model vs human).
              </p>
              <p class="tiny">
                Caution: detectors are imperfect; we treat this as a relative signal, not ground truth.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="callout">
        <strong>Planned metrics (next):</strong>
        <ul>
          <li><strong>Paper-groundedness / hallucination:</strong> claim-level support checking against the paper (supported / not-found / contradicted).</li>
          <li><strong>Coverage & prioritization:</strong> whether the review identifies and correctly ranks major issues (e.g., recall@K for major concerns).</li>
          <li><strong>Synthesizing ability (correctness):</strong> validate that external references/entities are real and relevant, not name-dropping.</li>
        </ul>
        <p class="tiny">These are required to answer our core questions on when LLM reviewers can surpass humans and what interventions improve failures.</p>
      </div>
    </div>
  </section>

  <!-- RESULTS -->
  <section class="section hero is-light" id="results">
    <div class="container is-max-desktop">
      <h2 class="title is-3 section-title has-text-centered">Preliminary Results (Similarity + Proxy Signals)</h2>

      <div class="content">
        <p>
          Below is an early snapshot used to validate the pipeline end-to-end. Numbers are not final; these results reflect similarity/proxy metrics and should not be interpreted as a full measure of review quality.
        </p>
      </div>

      <div class="table-wrap">
        <table class="table is-striped is-fullwidth">
          <thead>
            <tr>
              <th>Method</th>
              <th>Similarity (BLEU)</th>
              <th>Similarity (ROUGE-1 F1)</th>
              <th>Similarity (BERTScore)</th>
              <!-- <th>SEA Sync / Standardization</th> -->
              <th>LLM-as-a-Judge</th>
              <th>LLM Detector</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Human</strong></td>
              <td>0.615</td>
              <td>0.560</td>
              <td>0.851</td>
              <td>TODO</td>
              <td>TODO</td>
            </tr>
            <tr>
              <td><strong>Reviewer2</strong></td>
              <td>0.267</td>
              <td>0.36</td>
              <td>0.8326</td>
              <td>TODO</td>
              <td>1.139</td>
            </tr>
            <tr>
              <td><strong>SEA-E</strong></td>
              <td>0.3936</td>
              <td>0.4100</td>
              <td>0.8535</td>
              <td>TODO</td>
              <td>1.125</td>
            </tr>
            <tr>
              <td><strong>SWIF2T</strong></td>
              <td>0.135</td>
              <td>0.246</td>
              <td>0.820</td>
              <td>TODO</td>
              <td>1.135</td>
            </tr>
          </tbody>
        </table>
      </div>

      <!-- <div class="columns" style="margin-top: 1rem;">
        <div class="column">
          <div class="box">
            <h3 class="title is-5">Granite vs Reviewer2 (ICLR subset)</h3>
            <div class="content">
              <p>
                On 48 ICLR 2024 papers with a Reviewer2-style prompt (max-over-references evaluation):
              </p>
              <ul>
                <li><strong>IBM Granite 4.0 3B:</strong> BLEU ≈ 6.4, ROUGE-1 ≈ 0.44, ROUGE-2 ≈ 0.12, ROUGE-L ≈ 0.20.</li>
                <li><strong>Reviewer2:</strong> BLEU ≈ 16.9, ROUGE-1 ≈ 0.45, ROUGE-2 ≈ 0.14, ROUGE-L ≈ 0.23.</li>
              </ul>
              <p class="tiny">
                Takeaway: close ROUGE but lower BLEU suggests more paraphrasing (similar content, different wording).
                Current limitation: prototype stability depends on an external Granite server.
              </p>
            </div>
          </div>
        </div>

        <div class="column">
          <div class="box">
            <h3 class="title is-5">Text similarity snapshot (SEA-E vs IBM-3B)</h3>
            <div class="content">
              <p class="tiny">
                Example numbers observed in an early “text similarity” table (ROUGE-L / BERTScore / BLEU-4):
              </p>
              <ul class="tiny">
                <li><strong>SEA-E:</strong> ROUGE-L 0.1612, BLEU-4 0.0272</li>
                <li><strong>IBM-3B:</strong> ROUGE-L 0.1433, BLEU-4 0.0247</li>
              </ul>
              <p class="tiny">TODO: add the exact evaluation set definition + include confidence intervals.</p>
            </div>
          </div>
        </div>
      </div> -->

      <!-- Review comparison carousel -->
      <div class="review-compare">
        <div class="review-compare__head">
          <div>
            <h3 class="title is-5">Review comparison slider (3 papers)</h3>
            <p class="tiny">Slide through papers to compare Human vs SEA vs Reviewer2 reviews side-by-side.</p>
          </div>
          <div class="review-compare__legend">
            <span class="legend-chip legend-chip--human">Human</span>
            <span class="legend-chip legend-chip--sea">SEA</span>
            <span class="legend-chip legend-chip--review2">Reviewer2</span>
          </div>
        </div>

        <div class="carousel review-compare__carousel" data-autoplay="true" data-delay="7000" style="background: white;">
          <div class="item" data-paper-id="0A5o6dCKeK" style="display: flex !important; opacity: 1 !important; visibility: visible !important;">
            <div class="review-compare__slide" style="width: 100%; display: grid;">
              <div class="review-col" data-kind="human">
                <div class="review-col__head">
                  <span>Human Review</span>
                  <span class="review-col__meta">Paper 0A5o6dCKeK</span>
                </div>
                <div class="review-col__body" data-load="static/sample/0A5o6dCKeK/human.json" data-json-review-index="0"></div>
              </div>
              <div class="review-col" data-kind="sea">
                <div class="review-col__head">
                  <span>SEA Review</span>
                  <span class="review-col__meta">SEA.txt</span>
                </div>
                <div class="review-col__body" data-load="static/sample/0A5o6dCKeK/SEA.txt"></div>
              </div>
              <div class="review-col" data-kind="review2">
                <div class="review-col__head">
                  <span>Reviewer2</span>
                  <span class="review-col__meta">review2.txt</span>
                </div>
                <div class="review-col__body" data-load="static/sample/0A5o6dCKeK/review2.txt"></div>
              </div>
            </div>
          </div>

          <div class="item" data-paper-id="0aEUd9UtiA" style="display: flex !important; opacity: 1 !important; visibility: visible !important;">
            <div class="review-compare__slide" style="width: 100%; display: grid;">
              <div class="review-col" data-kind="human">
                <div class="review-col__head">
                  <span>Human Review</span>
                  <span class="review-col__meta">Paper 0aEUd9UtiA</span>
                </div>
                <div class="review-col__body" data-load="static/sample/0aEUd9UtiA/human.json" data-json-review-index="0"></div>
              </div>
              <div class="review-col" data-kind="sea">
                <div class="review-col__head">
                  <span>SEA Review</span>
                  <span class="review-col__meta">SEA.txt</span>
                </div>
                <div class="review-col__body" data-load="static/sample/0aEUd9UtiA/SEA.txt"></div>
              </div>
              <div class="review-col" data-kind="review2">
                <div class="review-col__head">
                  <span>Reviewer2</span>
                  <span class="review-col__meta">review2.txt</span>
                </div>
                <div class="review-col__body" data-load="static/sample/0aEUd9UtiA/review2.txt"></div>
              </div>
            </div>
          </div>

          <div class="item" data-paper-id="2dnO3LLiJ1" style="display: flex !important; opacity: 1 !important; visibility: visible !important;">
            <div class="review-compare__slide" style="width: 100%; display: grid;">
              <div class="review-col" data-kind="human">
                <div class="review-col__head">
                  <span>Human Review</span>
                  <span class="review-col__meta">Paper 2dnO3LLiJ1</span>
                </div>
                <div class="review-col__body" data-load="static/sample/2dnO3LLiJ1/human.json" data-json-review-index="0"></div>
              </div>
              <div class="review-col" data-kind="sea">
                <div class="review-col__head">
                  <span>SEA Review</span>
                  <span class="review-col__meta">SEA.txt</span>
                </div>
                <div class="review-col__body" data-load="static/sample/2dnO3LLiJ1/SEA.txt"></div>
              </div>
              <div class="review-col" data-kind="review2">
                <div class="review-col__head">
                  <span>Reviewer2</span>
                  <span class="review-col__meta">review2.txt</span>
                </div>
                <div class="review-col__body" data-load="static/sample/2dnO3LLiJ1/review2.txt"></div>
              </div>
            </div>
          </div>
        </div>
      </div>

    </div>
  </section>

  <!-- ROADMAP -->
  <section class="section" id="roadmap">
    <div class="container is-max-desktop">
      <h2 class="title is-3 section-title has-text-centered">Roadmap</h2>

      <div class="columns">
        <div class="column">
          <div class="box">
            <h3 class="title is-5">Next steps (Dec 30 - Jan 10)</h3>
            <div class="content">
              <ul>
                <li>Improve synthesizing-ability metric (entity extraction + scholarly verification robustness).</li>
                <li>Strengthen LLM-as-a-judge pipeline (prompt hardening, aggregation, diagnostics).</li>
                <li>Run at least one LLM detector per subgroup; report relative detectability.</li>
                <li>Modify the technical report/paper structure and finalize evaluation protocol.</li>
              </ul>
            </div>
          </div>
        </div>

        <div class="column">
          <div class="box">
            <h3 class="title is-5">Risks / limitations (current)</h3>
            <div class="content">
              <ul>
                <li><strong>Stability:</strong> external inference endpoints can introduce variability across runs.</li>
                <li><strong>Metric pitfalls:</strong> reference-based similarity does not capture correctness; hence judge + qualitative checks.</li>
                <li><strong>Synth score noise:</strong> entity disambiguation and acronym collisions can inflate/deflate scores.</li>
              </ul>
            </div>
          </div>
        </div>
      </div>

    </div>
  </section>

  <!-- RESOURCES -->
  <section class="section hero is-light" id="resources">
    <div class="container is-max-desktop">
      <h2 class="title is-3 section-title has-text-centered">Resources</h2>

      <div class="columns">
        <div class="column">
          <div class="box">
            <h3 class="title is-5">Code & Repro</h3>
            <div class="content">
              <ul>
                <li><strong>Repo:</strong> <a href="https://github.com/YOUR_GITHUB_USERNAME/YOUR_REPO" target="_blank">github.com/YOUR_GITHUB_USERNAME/YOUR_REPO</a></li>
                <li><strong>Pipeline:</strong> paper parsing → generation → SEA standardization → evaluation</li>
                <li><strong>Judge schema:</strong> strict JSON output for rubric scoring</li>
              </ul>
              <p class="tiny">TODO: add exact commands + environment once the repo is public.</p>
            </div>
          </div>
        </div>

        <div class="column">
            <div class="box">
              <h3 class="title is-5">Datasets</h3>
              <div class="content">
                  <ul>
                  <li><strong>Artifacts:</strong> raw PDFs; full paper content in structured text (e.g., <span class="mono">.mmd</span>); reviews & comments in JSON; raw reviews in text.</li>
                  <li><strong>Repo:</strong> <a href="https://github.com/YOUR_GITHUB_USERNAME/YOUR_REPO" target="_blank">github.com/YOUR_GITHUB_USERNAME/YOUR_REPO</a></li>
                </ul>
                <p class="tiny">TODO: add more details about the newly crawled dataset here - ICLR 25.</p>
              </div>
            </div>
          </div>

        <!-- <div class="column">
          <div class="box">
            <h3 class="title is-5">Related references we build on</h3>
            <div class="content tiny">
              <ul>
                <li><a href="https://arxiv.org/abs/2407.12857v2" target="_blank">A paper discussing relevance-style metrics (arXiv:2407.12857v2)</a></li>
                <li>Reviewer2-style prompt→review pipeline (prompt generation + templated review sections)</li>
                <li>Focus-level evaluation tables (text similarity baselines) used as a sanity check</li>
              </ul>
              <p class="tiny">TODO: replace with the final curated bibliography list.</p>
            </div>
          </div>
        </div> -->
      </div>

      <div class="callout">
        <strong>Contact.</strong>
        If you want to collaborate or run new models on the benchmark, open an issue in the repo (once public) or email the team.
      </div>
    </div>
  </section>

    <!-- REFERENCES -->
    <section class="section" id="references">
      <div class="container is-max-desktop">
        <h2 class="title is-3 section-title has-text-centered">References</h2>
        <div class="content">
          <ol>
            <li>
              <strong>ICML Reviewer Instructions (2025).</strong>
              <a href="https://icml.cc/Conferences/2025/ReviewerInstructions" target="_blank" rel="noopener">
                https://icml.cc/Conferences/2025/ReviewerInstructions
              </a>
            </li>
            <li>
              <strong>Ollama Cloud Documentation.</strong>
              <a href="https://docs.ollama.com/cloud" target="_blank" rel="noopener">
                https://docs.ollama.com/cloud
              </a>
            </li>
            <li>
              <strong>Reviewer2.</strong>
              <span class="tiny">
                <a href="https://arxiv.org/pdf/2402.10886" target="_blank" rel="noopener">
                    https://arxiv.org/pdf/2402.10886
                </a>
              </span>
            </li>
            <li>
              <strong>SEA.</strong>
              <span class="tiny">
                <a href="https://arxiv.org/pdf/2407.12857v2" target="_blank" rel="noopener">
                    https://arxiv.org/pdf/2407.12857v2
                </a>
              </span>
            </li>
            <li>
                <strong>SWIF2T.</strong>
                <span class="tiny">
                    <a href="https://arxiv.org/pdf/2405.20477" target="_blank" rel="noopener">
                        https://arxiv.org/pdf/2405.20477
                    </a>
                </span>
            </li>
            <li>
              <strong>LLM detectors (human-likeness proxy):</strong> GPTZero, DetectGPT.
              <span class="tiny">Add the exact tools/versions and URLs used in your experiments.</span>
            </li>
            <li>
              <strong>IBM Granite 4.0 (baseline model used in preliminary runs).</strong>
              <span class="tiny">Add official model card/URL once you decide what to cite.</span>
            </li>
          </ol>
          <p class="tiny">
            Notes: We keep references minimal here until the technical report is public. Once your arXiv is ready,
            replace the TODO items with canonical citations and model/tool versioning.
          </p>
        </div>
      </div>
    </section>


  <!-- BIBTEX -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header" style="display:flex; justify-content:space-between; align-items:center;">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn button is-small is-link is-light" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>&nbsp;<span class="copy-text">Copy</span>
        </button>
      </div>

      <pre id="bibtex-code"><code>@article{urop2025_llms_as_reviewers,
  title   = {LLM Reviewers: When They Can Surpass Humans, and What Still Fails},
  author  = {UROP 2025 LLM Reviewers Team},
  journal = {Technical report (in progress)},
  year    = {2025},
  url     = {https://YOUR_GITHUB_USERNAME.github.io/YOUR_REPO/}
}</code></pre>

      <p class="tiny">
        TODO: update authors, venue, and identifier (arXiv / DOI) when available.
      </p>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>
              (adapted from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>).
              Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <script>
    // Minimal helpers if not provided by index.js
    function scrollToTop(){ window.scrollTo({top: 0, behavior: 'smooth'}); }

    function copyBibTeX() {
      const el = document.getElementById("bibtex-code");
      const text = el.innerText;
      if (!navigator.clipboard) {
        alert("Clipboard not available. Please copy manually.");
        return;
      }
      navigator.clipboard.writeText(text).then(() => {
        const btn = document.querySelector(".copy-bibtex-btn .copy-text");
        if (btn) { btn.textContent = "Copied"; setTimeout(()=>btn.textContent="Copy", 1200); }
      }).catch(() => alert("Copy failed. Please copy manually."));
    }
  </script>

</body>
</html>
