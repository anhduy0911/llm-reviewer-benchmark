 **Summary:**
The paper addresses the issue of artifacts in feature maps of Vision Transformers (ViTs), particularly in DINOv2 models. The authors propose a solution by adding register tokens to the input sequence, which are not used for outputs, thereby fixing the artifacts and improving performance on dense prediction tasks. The methodology involves a detailed analysis of the norms of tokens in different layers and their impact on model performance. The paper also introduces a novel approach to mitigate the effects of these artifacts by adding additional tokens, which are discarded after processing. The methodology is validated through experiments on various datasets, showing improved performance in dense prediction tasks and object discovery.

**Strengths:**
- The paper is well-written and easy to follow, making it accessible to a broad audience.
- The proposed method is simple, effective, and can be applied to any vision transformer, making it a versatile solution.
- The paper provides a detailed analysis of the artifacts in feature maps of ViTs, which is crucial for understanding the behavior of these models.
- The proposed solution of adding register tokens is innovative and has been validated through experiments, showing improved performance in dense prediction tasks and object discovery.
- The paper is well-organized, with clear figures and tables that aid in understanding the results and methodologies.

**Weaknesses:**
- The paper lacks a thorough comparison with existing methods, particularly those that address similar issues in other models or datasets.
- The paper does not provide a detailed analysis of the computational cost or the impact of the additional tokens on the training time.
- There is a lack of discussion on the potential negative societal impacts of the proposed method.
- The paper does not adequately address the limitations of the proposed method, such as its applicability to other models or datasets.
- The paper does not provide sufficient evidence to support the claim that the artifacts are due to the model discarding local patch information.
- The paper does not discuss the potential impact of the proposed method on the performance of other tasks, such as object detection, which could be affected by the discarding of local patch information.

**Questions:**
- Can the authors provide more details on the computational cost and training time implications of adding register tokens?
- How does the proposed method compare to other methods that address similar issues in other models or datasets?
- Is there a possibility that the artifacts are not solely due to the discarding of local patch information but could be influenced by other factors?
- How does the proposed method impact the performance of other tasks, such as object detection, which rely on local patch information?
- Could the authors provide more evidence to support the claim that the artifacts are due to the discarding of local patch information?
- Is there a potential for the proposed method to be applied to other models or datasets, and if so, what are the expected outcomes?

**Soundness:**
3 good

**Presentation:**
3 good

**Contribution:**
3 good

**Rating:**
7 accept, but needs minor improvements

**Paper Decision:**
- Decision: Accept
- Reasons: The paper presents a novel and effective solution to a significant problem in ViT models, namely the presence of artifacts in feature maps. The proposed method of adding register tokens to the input sequence is validated through experiments and shown to improve performance on dense prediction tasks and object discovery. The paper is well-written, easy to follow, and contributes to the understanding of ViT models. However, the paper could benefit from a more thorough comparison with existing methods, a discussion on the computational cost, and a more detailed analysis of the limitations and potential negative societal impacts of the proposed method. The decision to accept is based on the paper's originality, methodological soundness, and the significance of its results, as well as its clarity and logic of presentation.